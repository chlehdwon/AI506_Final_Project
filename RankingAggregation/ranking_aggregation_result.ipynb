{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from https://github.com/jianboli/HyperGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T22:58:52.164757Z",
     "start_time": "2019-05-11T22:58:52.154913Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import weightedtau, kendalltau\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import null_space\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import trueskill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse game data: Free-For-All / 1-v-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T22:59:18.581419Z",
     "start_time": "2019-05-11T22:58:54.012537Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_str = '%d %B %Y %H:%M:%S'\n",
    "\n",
    "dt_lim = datetime.strptime('06 August 2004 18:13:50', dt_str) #first game in HeadToHead\n",
    "\n",
    "players = set()\n",
    "\n",
    "# each entry is a tuple ([list of players], [list of scores])\n",
    "matches = []\n",
    "\n",
    "# needed for iteration\n",
    "cur_game = -1\n",
    "cur_players = []\n",
    "cur_scores = []\n",
    "\n",
    "# also, filter out all games where every player has no score!\n",
    "with open('FreeForAll.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        date = datetime.strptime(row[0], dt_str)\n",
    "        score = int(row[6])\n",
    "        if date < dt_lim:\n",
    "            game = int(row[1])\n",
    "            player = row[4]\n",
    "            score = int(row[6])\n",
    "            \n",
    "            # next, decide if this row is from the same match\n",
    "            # as the last row, or a different match\n",
    "            if game == cur_game:\n",
    "                cur_players.append(player)\n",
    "                cur_scores.append(score)\n",
    "            else:\n",
    "                if cur_game > 0 and np.sum(np.abs(cur_scores)):\n",
    "                    # append cur_players, cur_scores to matches\n",
    "                    matches.append((cur_players, cur_scores))\n",
    "                    # add cur_players to players\n",
    "                    players.update(cur_players)\n",
    "\n",
    "                # reset cur_game, cur_players, cur_scores\n",
    "                cur_game = game\n",
    "                cur_players = [player]\n",
    "                cur_scores = [score]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "players=list(players) # list of players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31028\n",
      "5507\n",
      "0.0011200307862303486\n",
      "6.16800953977053\n"
     ]
    }
   ],
   "source": [
    "print(len(matches))\n",
    "print(len(players))\n",
    "\n",
    "avg_size = 0\n",
    "nnz = 0\n",
    "for i in range(len(matches)):\n",
    "    pi, score = matches[i]\n",
    "    nnz += len(pi)\n",
    "    avg_size += len(pi)\n",
    "print(nnz / (len(matches) * len(players)))\n",
    "print(avg_size / len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Hypergraph\n",
    "\n",
    "Then, train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "outputdir = \"../downstreamdata/halo/\"\n",
    "if os.path.isdir(outputdir) is False:\n",
    "    os.makedirs(outputdir)\n",
    "\n",
    "outputpath = outputdir + \"hypergraph.txt\"\n",
    "outputpath2 = outputdir + \"hypergraph_pos.txt\"\n",
    "\n",
    "with open(outputpath, \"w\") as f, open(outputpath2, \"w\") as sf:\n",
    "    for i in range(len(matches)):\n",
    "        pi, scores = matches[i]\n",
    "        if len(pi) > 1: \n",
    "            line = [str(v) for v in pi]\n",
    "            line2 = [str(w) for w in scores]\n",
    "            f.write(\"\\t\".join(line) + \"\\n\")\n",
    "            sf.write(\"\\t\".join(line2) + \"\\n\")\n",
    "            \n",
    "train_hindex, valid_hindex = train_test_split(range(len(matches)), test_size=0.25, random_state=21)\n",
    "with open(outputdir + \"valid_hindex.txt\", \"w\") as f:\n",
    "    for h in valid_hindex:\n",
    "        f.write(str(h) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for computing PageRank rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T22:59:18.592065Z",
     "start_time": "2019-05-11T22:59:18.585014Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# COMPUTE PAGERANK\n",
    "##################################################\n",
    "\n",
    "# given probability transition matrix P\n",
    "# where P_{v,w} = Prob(w -> v)\n",
    "# find pagerank scores with restart probability r\n",
    "def compute_pr(P, r, n, eps=1e-8):\n",
    "    x = np.ones(n) / n*1.0\n",
    "    flag = True\n",
    "    t=0\n",
    "    while flag:\n",
    "        x_new = (1-r)*P*x\n",
    "        x_new = x_new + np.ones(n) * r / n\n",
    "        diff = np.linalg.norm(x_new - x)\n",
    "        if np.linalg.norm(x_new - x,ord=1) < eps and t > 100:\n",
    "            flag = False\n",
    "        t=t+1\n",
    "        x = x_new\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Hypergraph rankings: Hypergraph w/ GroundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rankings are a |V| x 1 vector, where the v-th entry is the PageRank score (or TrueSkill rating)  of vertex v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:01:52.900105Z",
     "start_time": "2019-05-11T23:00:57.558053Z"
    }
   },
   "outputs": [],
   "source": [
    "pi_list = matches\n",
    "universe = np.array(list(players))\n",
    "# first create these matrices\n",
    "# R = |E| x |V|, R(e, v) = lambda_e(v)\n",
    "# W = |V| x |E|, W(v, e) = w(e) 1(v in e)\n",
    "    \n",
    "m = len(pi_list) # number of hyperedges\n",
    "n = len(universe) # number of items to be ranked \n",
    "R = np.zeros([m, n])\n",
    "W = np.zeros([n, m])\n",
    "\n",
    "for i in range(len(pi_list)):\n",
    "    pi, scores = pi_list[i]\n",
    "    if len(pi) > 1:   \n",
    "        for j in range(len(pi)):\n",
    "            v = pi[j]\n",
    "            v = np.where(universe == v)[0][0] #equivalent to universe.index(v) but for np arrays\n",
    "            R[i, v] = np.exp(scores[j])\n",
    "            W[v,i] = (np.std(scores) + 1.0)\n",
    "        R[i, :] = R[i,:] / sum(R[i,:])\n",
    "\n",
    "# first, normalize W\n",
    "Wnorm=W/W.sum(axis=1)[:,None]\n",
    "Ws = sparse.csr_matrix(Wnorm)\n",
    "Rs = sparse.csr_matrix(R)\n",
    "\n",
    "# create prob trans matrices\n",
    "P = np.transpose(Ws.dot(Rs))\n",
    "\n",
    "# create rankings\n",
    "r=0.40\n",
    "rankings_hg = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G^H rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:09:01.371821Z",
     "start_time": "2019-05-11T23:08:36.283463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create matrix A, where A_{u,v} is given in Eq 10\n",
    "def compute_gh_weights(R, W, P):\n",
    "    E, V = R.shape\n",
    "    A = np.zeros([V,V]) # to return\n",
    "    \n",
    "    # first, create edge weight vector\n",
    "    WE = np.zeros(E)\n",
    "    # for each edge, find first non-zero value that is >0\n",
    "    for e in range(E):\n",
    "        WE[e] = W[np.where(W[:,e] > 0)[0],e][0]\n",
    "    \n",
    "    # iterate over edges, add w(e) * gam_e(u) * gam_e(v) term\n",
    "    # for each pair of vertices u,v \\in e\n",
    "    for e in range(E):\n",
    "        nodes_in_e = np.nonzero(R[e,:])[0]\n",
    "        for u in nodes_in_e:\n",
    "            for v in nodes_in_e:\n",
    "                A[u,v] += WE[e] * R[e,u] * R[e,v]\n",
    "    return A\n",
    "\n",
    "# create A, then find pagerank scores of random walk on A\n",
    "\n",
    "# get probability transition matrix\n",
    "A=compute_gh_weights(R, W, P)\n",
    "P = A/A.sum(axis=1)[:,None]\n",
    "P=P.T \n",
    "P = sparse.csr_matrix(P)\n",
    "\n",
    "# compute pagerank scores\n",
    "r=0.40\n",
    "rankings_gh = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC3 rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:12:39.785260Z",
     "start_time": "2019-05-11T23:10:47.456019Z"
    }
   },
   "outputs": [],
   "source": [
    "n = len(universe)\n",
    "\n",
    "Pd = np.zeros([n, n]) # d for dwork\n",
    "\n",
    "for i in universe:\n",
    "    i_counts = np.zeros(n) # number of ways i can go to any other vertex\n",
    "    i_deg = 0 #number of hyperedges where i can traverse to some other vertex\n",
    "\n",
    "    i_index = np.where(universe == i)\n",
    "    for pi, scores in pi_list:\n",
    "        if i in pi and len(pi) > 1:\n",
    "            pi_filtered = pi[pi.index(i)+1:] #everything ranked better than i\n",
    "\n",
    "            # if i can use this hyperedge\n",
    "            if len(pi_filtered) > 0:\n",
    "                # essentially, for each j in pi_filtered\n",
    "                # grab k=universe.index(j) and increment i_counts[k] by 1/len(pi)\n",
    "                i_counts[np.where(np.isin(universe, pi_filtered))] += 1/len(pi)\n",
    "\n",
    "            i_counts[i_index] += 1 - (len(pi_filtered) / len(pi))\n",
    "            i_deg += 1\n",
    "    if i_deg > 0:\n",
    "        i_counts /= i_deg\n",
    "    else:\n",
    "        i_counts[i_index] = 1\n",
    "    Pd[i_index,:] = i_counts\n",
    "\n",
    "Pd = np.transpose(Pd) # since we're using column vectors\n",
    "Pd = sparse.csr_matrix(Pd)\n",
    "\n",
    "# create MC3 rankings\n",
    "r=0.40\n",
    "\n",
    "rankings_mc3 = compute_pr(Pd, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrueSkill rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:15:48.240537Z",
     "start_time": "2019-05-11T23:15:48.220992Z"
    }
   },
   "outputs": [],
   "source": [
    "# simulate the change in TrueSkill ratings when a Free-For-All match is played\n",
    "def play_match(match, ts_ranking):\n",
    "    p, s = match\n",
    "    cur_ranks = []\n",
    "    for player in p:\n",
    "        if player in ts_ranking:\n",
    "            cur_ranks.append([ts_ranking[player]])\n",
    "        else:\n",
    "            cur_ranks.append([trueskill.Rating()])\n",
    "    # lower rank = better player for trueskill.rate function, so we turn scores into -1*scores\n",
    "    match_res = trueskill.rate(cur_ranks, ranks=[-1*i for i in s])\n",
    "    for i in range(len(p)):\n",
    "        player = p[i]\n",
    "        ts_ranking[player] = match_res[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:17:12.247092Z",
     "start_time": "2019-05-11T23:15:48.827252Z"
    }
   },
   "outputs": [],
   "source": [
    "trueskill_rankings={} # dict mapping player -> TrueSkill rating object\n",
    "\n",
    "# simulate all matches being played, in order\n",
    "for match in matches:\n",
    "    play_match(match, trueskill_rankings)\n",
    "\n",
    "rankings_ts = [trueskill_rankings[player].mu for player in players] # deterministic TrueSkill ratings list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unifrom Weight rankings: Hypergraph w/o Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_list = matches\n",
    "universe = np.array(list(players))\n",
    "# first create these matrices\n",
    "# R = |E| x |V|, R(e, v) = lambda_e(v)\n",
    "# W = |V| x |E|, W(v, e) = w(e) 1(v in e)\n",
    "    \n",
    "m = len(pi_list) # number of hyperedges\n",
    "n = len(universe) # number of items to be ranked \n",
    "R = np.zeros([m, n])\n",
    "W = np.zeros([n, m])\n",
    "\n",
    "for i in range(len(pi_list)):\n",
    "    pi, scores = pi_list[i]\n",
    "    if len(pi) > 1:   \n",
    "        for j in range(len(pi)):\n",
    "            v = pi[j]\n",
    "            v = np.where(universe == v)[0][0] #equivalent to universe.index(v) but for np arrays\n",
    "            R[i, v] = 1.0 # uniform weight\n",
    "            W[v,i] = 1.0\n",
    "        R[i, :] = R[i,:] / sum(R[i,:])\n",
    "\n",
    "# first, normalize W\n",
    "Wnorm=W/W.sum(axis=1)[:,None]\n",
    "Ws = sparse.csr_matrix(Wnorm)\n",
    "Rs = sparse.csr_matrix(R)\n",
    "\n",
    "# create prob trans matrices\n",
    "P = np.transpose(Ws.dot(Rs))\n",
    "\n",
    "# create rankings\n",
    "r=0.40\n",
    "rankings_unif = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning Weight rankings: Hypergraph w/ WHATsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"../train_results/halo/\"# path for saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_list = matches\n",
    "universe = np.array(list(players))\n",
    "# first create these matrices\n",
    "# R = |E| x |V|, R(e, v) = lambda_e(v)\n",
    "# W = |V| x |E|, W(v, e) = w(e) 1(v in e)\n",
    "    \n",
    "m = len(pi_list) # number of hyperedges\n",
    "n = len(universe) # number of items to be ranked \n",
    "R = np.zeros([m, n])\n",
    "W = np.zeros([n, m])\n",
    "\n",
    "predict_scores = []\n",
    "with open(outputdir + \"prediction.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        scores = [float(s) for s in line.rstrip().split(\"\\t\")]\n",
    "        predict_scores.append(scores)\n",
    "\n",
    "for i in range(len(pi_list)):\n",
    "    pi = pi_list[i][0]\n",
    "    scores = predict_scores[i]\n",
    "    assert len(pi) == len(scores)\n",
    "    \n",
    "    if len(pi) > 1:   \n",
    "        binned_scores = []\n",
    "        for j in range(len(pi)):\n",
    "            v = pi[j]\n",
    "            v = np.where(universe == v)[0][0]\n",
    "            sc = scores[j]\n",
    "            R[i, v] = sc \n",
    "            W[v,i] = 1.0\n",
    "            binned_scores.append(np.log(sc))\n",
    "        W[:, i] = (np.std(binned_scores) + 1.0) * W[:, i]\n",
    "        R[i, :] = R[i,:] / sum(R[i,:])\n",
    "\n",
    "# first, normalize W\n",
    "Wnorm=W/W.sum(axis=1)[:,None]\n",
    "Ws = sparse.csr_matrix(Wnorm)\n",
    "Rs = sparse.csr_matrix(R)\n",
    "\n",
    "# create prob trans matrices\n",
    "P = np.transpose(Ws.dot(Rs))\n",
    "\n",
    "# create rankings\n",
    "r=0.40\n",
    "rankings_bw = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:17:20.012311Z",
     "start_time": "2019-05-11T23:17:19.982816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating a 1v1 game with a deterministic ranking of players\n",
    "# INPUTS:\n",
    "# game_players: list of players in the match\n",
    "# game_scores: list of scores in the match (corresponding to game_players)\n",
    "# all_players: list of all players in all matches\n",
    "# ranks: one of the 4 rankings computed above\n",
    "\n",
    "# OUTPUT: \n",
    "# can_eval: False if game ends in tie, True otherwise (we ignore tie games)\n",
    "# res: 1 if ranks correctly predicts the match, 0 if not\n",
    "def eval_game_h2h(game_players, game_scores, all_players, ranks):\n",
    "    players_ranked_prev = [player for player in game_players if player in all_players]\n",
    "    if len(players_ranked_prev) == 2:\n",
    "        # get scores for players previously ranked\n",
    "        scores_prev = [game_scores[game_players.index(player)] for player in players_ranked_prev]\n",
    "        ranks_prev = [ranks[all_players.index(player)] for player in players_ranked_prev]\n",
    "\n",
    "        # make sure there isn't a tie\n",
    "        if scores_prev[0] != scores_prev[1]:\n",
    "            can_eval = True\n",
    "            \n",
    "            # check if ranked correctly\n",
    "            if sum(np.argsort(scores_prev) == np.argsort(ranks_prev)) == 2:\n",
    "                res = True\n",
    "            else:\n",
    "                res = False\n",
    "        else:\n",
    "            can_eval = False\n",
    "            res = False\n",
    "    else:\n",
    "        can_eval = False\n",
    "        res = False\n",
    "    return (can_eval, int(res))\n",
    "\n",
    "# Evaluating a 1v1 game with TS probabilistic procedure. Same inputs/outputs as above.\n",
    "def eval_game_h2h_trueskill(game_players, game_scores, all_players, ts_ranking):\n",
    "    players_ranked_prev = [player for player in game_players if player in all_players]\n",
    "    if len(players_ranked_prev) == 2:\n",
    "        # get scores for players previously ranked\n",
    "        scores_prev = [game_scores[game_players.index(player)] for player in players_ranked_prev]\n",
    "        ts_ranks_prev = [ts_ranking[player] for player in players_ranked_prev]\n",
    "\n",
    "        # make sure there isn't a tie\n",
    "        if scores_prev[0] != scores_prev[1]:\n",
    "            can_eval = True\n",
    "            \n",
    "            # compare rating distributions between two players \n",
    "            # (for simplicity, do not consider draw probability)\n",
    "            mu0, sigma0 = ts_ranks_prev[0]\n",
    "            mu1, sigma1 = ts_ranks_prev[1]\n",
    "            p = 1 - norm.cdf(-1.0 * (mu0 - mu1) / (sigma0**2 + sigma1**2))\n",
    "            if (p > 0.5 and scores_prev[0] > scores_prev[1]) or (p < 0.5 and scores_prev[0] < scores_prev[1]):\n",
    "                res = True\n",
    "            else:\n",
    "                res = False\n",
    "\n",
    "        else:\n",
    "            can_eval = False\n",
    "            res = False\n",
    "    else:\n",
    "        can_eval = False\n",
    "        res = False\n",
    "    return (can_eval, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through each game and use each of the different rankings to predict the winner. Compare to actual winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:17:44.757611Z",
     "start_time": "2019-05-11T23:17:24.344544Z"
    }
   },
   "outputs": [],
   "source": [
    "cur_game = -1\n",
    "cur_players = []\n",
    "cur_scores = []\n",
    "\n",
    "results_hg=[]\n",
    "results_gh=[]\n",
    "results_mc3=[]\n",
    "results_ts=[]\n",
    "results_ts_prob=[]\n",
    "# additional\n",
    "results_unif=[]\n",
    "results_bw=[]\n",
    "\n",
    "with open('HeadToHead.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        game = int(row[1])\n",
    "        player = row[4]\n",
    "        score = int(row[6])\n",
    "\n",
    "        # next, decide if this row is from the same match\n",
    "        # as the last row, or a different match\n",
    "        if game == cur_game:\n",
    "            cur_players.append(player)\n",
    "            cur_scores.append(score)\n",
    "        else:\n",
    "            if cur_game > 0 and np.sum(np.abs(cur_scores)) > 0:\n",
    "                # evaluate game\n",
    "                can_eval, hg_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_hg)\n",
    "                can_eval, gh_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_gh)\n",
    "                can_eval, mc3_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_mc3)\n",
    "                can_eval, ts_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_ts)\n",
    "                # additional\n",
    "                can_eval, unif_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_unif)\n",
    "                can_eval, bw_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_bw)\n",
    "                \n",
    "                can_eval, ts_prob_match_res = eval_game_h2h_trueskill(cur_players, cur_scores, players, trueskill_rankings)\n",
    "                \n",
    "                if can_eval:\n",
    "                    results_hg.append(hg_match_res)\n",
    "                    results_gh.append(gh_match_res)\n",
    "                    results_mc3.append(mc3_match_res)\n",
    "                    results_ts.append(ts_match_res)\n",
    "                    \n",
    "                    results_ts_prob.append(ts_prob_match_res)\n",
    "                    \n",
    "                    results_unif.append(unif_match_res)\n",
    "                    results_bw.append(bw_match_res)\n",
    "\n",
    "            # reset cur_game, cur_players, cur_scores\n",
    "            cur_game = game\n",
    "            cur_players = [player]\n",
    "            cur_scores = [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:30.075928Z",
     "start_time": "2019-05-11T23:18:30.059243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergraph w/ GroundTruth accuracy: 0.7113685450618495\n",
      "Clique Graph accuracy: 0.6112311015118791\n",
      "Dwork MC3 accuracy: 0.5293540153151384\n",
      "TrueSkill accuracy: 0.7345376006283134\n",
      "TrueSkill accuracy, probabilistic decision procedure: 0.7345376006283134\n",
      "Hypergraph w/o Labels accuracy: 0.5352444531710191\n",
      "Hypergraph w/ WHATsNet accuracy: 0.7180443746318477\n"
     ]
    }
   ],
   "source": [
    "num_games = len(results_hg)\n",
    "print('Hypergraph w/ GroundTruth accuracy: {}'.format(sum(results_hg) * 1.0 / num_games))\n",
    "print('Clique Graph accuracy: {}'.format(sum(results_gh) * 1.0 / num_games))\n",
    "print('Dwork MC3 accuracy: {}'.format(sum(results_mc3) * 1.0 / num_games))\n",
    "print('TrueSkill accuracy: {}'.format(sum(results_ts) * 1.0 / num_games))\n",
    "print('TrueSkill accuracy, probabilistic decision procedure: {}'.format(sum(results_ts_prob) * 1.0 / num_games))\n",
    "print('Hypergraph w/o Labels accuracy: {}'.format(sum(results_unif) * 1.0 / num_games))\n",
    "print('Hypergraph w/ WHATsNet accuracy: {}'.format(sum(results_bw) * 1.0 / num_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences from TS rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:44.212622Z",
     "start_time": "2019-05-11T23:18:44.201501Z"
    }
   },
   "outputs": [],
   "source": [
    "hg_only=0\n",
    "ts_only=0\n",
    "both=0\n",
    "for i in range(num_games):\n",
    "    if results_hg[i] > 0 and results_ts[i] > 0:\n",
    "        both += 1\n",
    "    elif results_hg[i] > 0:\n",
    "        hg_only += 1\n",
    "    elif results_ts[i] > 0:\n",
    "        ts_only += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:44.832459Z",
     "start_time": "2019-05-11T23:18:44.825829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of Matches predicted correctly by both TrueSkill and hypergraph w/ GroundTruth: 0.6220302375809935\n",
      "% of Matches predicted correctly by only TrueSkill: 0.11250736304731986\n",
      "% of Matches predicted correctly by only hypergraph w/ GroundTruth: 0.08933830748085608\n"
     ]
    }
   ],
   "source": [
    "print('% of Matches predicted correctly by both TrueSkill and hypergraph w/ GroundTruth: {}'.format(both / num_games))\n",
    "print('% of Matches predicted correctly by only TrueSkill: {}'.format(ts_only / num_games))\n",
    "print('% of Matches predicted correctly by only hypergraph w/ GroundTruth: {}'.format(hg_only / num_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:44.212622Z",
     "start_time": "2019-05-11T23:18:44.201501Z"
    }
   },
   "outputs": [],
   "source": [
    "bw_only=0\n",
    "ts_only=0\n",
    "both=0\n",
    "for i in range(num_games):\n",
    "    if results_bw[i] > 0 and results_ts[i] > 0:\n",
    "        both += 1\n",
    "    elif results_bw[i] > 0:\n",
    "        bw_only += 1\n",
    "    elif results_ts[i] > 0:\n",
    "        ts_only += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:44.832459Z",
     "start_time": "2019-05-11T23:18:44.825829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of Matches predicted correctly by both TrueSkill and hypergraph w/ WHATsNet: 0.6342038091498134\n",
      "% of Matches predicted correctly by only TrueSkill: 0.1003337914784999\n",
      "% of Matches predicted correctly by only hypergraph w/ WHATsNet: 0.08384056548203417\n"
     ]
    }
   ],
   "source": [
    "print('% of Matches predicted correctly by both TrueSkill and hypergraph w/ WHATsNet: {}'.format(both / num_games))\n",
    "print('% of Matches predicted correctly by only TrueSkill: {}'.format(ts_only / num_games))\n",
    "print('% of Matches predicted correctly by only hypergraph w/ WHATsNet: {}'.format(bw_only / num_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Full on Python 3.7 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
